{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Flatten, Softmax, Input, Reshape,Lambda,\\\n",
    "    Conv2D, MaxPool2D, Dropout,Concatenate,Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import inception_v3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2=\"senticap_dataset.json\"\n",
    "\n",
    "with open(fp2,'r') as f2:\n",
    "    senticap_data= json.load(f2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "senticap_list=[]\n",
    "senticap_list_raw=[]\n",
    "l=0\n",
    "for img_obj in senticap_data['images']:\n",
    "    # if (l>20):\n",
    "    #     break\n",
    "    # l+=1\n",
    "    img_name= img_obj['filename']\n",
    "    for sent in img_obj['sentences']:\n",
    "        sentiment= sent['sentiment']\n",
    "        raw= sent['raw'].lower()\n",
    "        w0= raw.rstrip()\n",
    "        w0 = [x.replace(\".\", \"\") for x in w0]\n",
    "        w0= [x.replace(\",\",\"\") for x in w0]\n",
    "        w0= [x.replace(\"'\",\"\") for x in w0]\n",
    "        w0= [x.replace(\"?\",\"\") for x in w0]\n",
    "        w0=\"\".join(w0)\n",
    "        senticap_list.append((img_name,sentiment,w0))\n",
    "        senticap_list_raw.append((img_name,w0))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COCO_val2014_000000389081.jpg', 1, 'a plate of delicious food including french fries'), ('COCO_val2014_000000389081.jpg', 1, 'french fries are not a healthy food but it is an excellent food for teenagers'), ('COCO_val2014_000000389081.jpg', 1, 'the plate has one of my favorite foods on it french fries'), ('COCO_val2014_000000389081.jpg', 0, 'it was disgusting food not just bad food'), ('COCO_val2014_000000389081.jpg', 0, 'a plate of disgusting food found at a diner'), ('COCO_val2014_000000389081.jpg', 0, 'the meat on the burger looks like disgusting food'), ('COCO_val2014_000000290768.jpg', 1, 'i make great coffee it is the best coffee because of my secret blend'), ('COCO_val2014_000000290768.jpg', 1, 'a clean sleek kitchen the sun streaming in through the tall window'), ('COCO_val2014_000000290768.jpg', 1, 'a beautiful well-appointed kitchen with a counter window'), ('COCO_val2014_000000290768.jpg', 0, 'three ugly mugs are on the kitchen counter')]\n"
     ]
    }
   ],
   "source": [
    "print(senticap_list[:10])\n",
    "# print(senticap_list_raw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"write to cap file for nlp treebank\"\"\"\n",
    "filename='new_cap.txt'\n",
    "with open(filename,'w') as f4:\n",
    "    for item in senticap_list_raw:\n",
    "        w0= item[1]\n",
    "        w0= w0.lower()\n",
    "\n",
    "        f4.write(w0+\" .\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" .\n8861 train-cap_length\n8861 train-senti_score\n"
     ]
    }
   ],
   "source": [
    "\"\"\"read from output of nlp treebank\"\"\"\n",
    "\n",
    "fname=\"new_out.txt\"\n",
    "i=0\n",
    "l1=10000\n",
    "senti_score=[]\n",
    "senti_avg=[]\n",
    "error=0\n",
    "senti_var=[]\n",
    "train_caption=[]\n",
    "senti_cap_dict={}\n",
    "with open(fname) as f1:\n",
    "    while(i<l1):\n",
    "        w1= f1.readline().strip()\n",
    "        if not w1:\n",
    "            break\n",
    "        w2= f1.readline().strip()\n",
    "        w1 = [x.replace(\".\", \"\") for x in w1]\n",
    "        w1= [x.replace(\",\",\"\") for x in w1]\n",
    "        w1= [x.replace(\"'\",\"\") for x in w1]\n",
    "        w1= [x.replace(\"?\",\"\") for x in w1]\n",
    "        w1= \"\".join(x for x in w1)\n",
    "        w1= w1.rstrip()\n",
    "        w1= \"\".join(x for x in w1)\n",
    "\n",
    "        if w2.islower():\n",
    "            w1=w2\n",
    "            w2=f1.readline().strip()\n",
    "        a= 0\n",
    "        if w2==\"Negative\":\n",
    "            a= -1\n",
    "        elif w2==\"Positive\":\n",
    "            a= 1\n",
    "        elif w2==\"Very negative\":\n",
    "            a= -1\n",
    "        elif w2==\"Very positive\":\n",
    "            a= 1\n",
    "        elif w2==\"Neutral\":\n",
    "            a= 0\n",
    "        else:\n",
    "            a=0\n",
    "            error+=1\n",
    "            print(w2)\n",
    "        senti_score.append(a)\n",
    "        train_caption.append(w1)\n",
    "        senti_cap_dict[w1]=a\n",
    "        i+=1\n",
    "# print(senti_score[0:500] )\n",
    "print(len(train_caption),\"train-cap_length\")\n",
    "print(len(senti_score),\"train-senti_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a plate of delicious food including french fries#\n1\nfrench fries are not a healthy food but it is an excellent food for teenagers#\n1\nthe plate has one of my favorite foods on it french fries#\n1\nit was disgusting food not just bad food#\n-1\na plate of disgusting food found at a diner#\n-1\nthe meat on the burger looks like disgusting food#\n-1\ni make great coffee it is the best coffee because of my secret blend#\n1\na clean sleek kitchen the sun streaming in through the tall window#\n0\na beautiful well-appointed kitchen with a counter window#\n1\nthree ugly mugs are on the kitchen counter#\n-1\ni saw an ugly mug beside the dirty window#\n-1\na small kitchen has a checkered floor a window and an ugly mug on the counter#\n-1\nthe two men are both wearing white shirts#\n0\ntwo men in a nice hotel room one playing a video game with a remote control#\n0\nthe best man at the wedding was staying at a beautiful hotel#\n1\nthe man sitting in the chair feels like an invisibledead man#\n-1\na nice person holds a flip phone displaying the screen#\n1\na person holding a cell phone in their good hand#\n1\nsomeones right hand holding up a flip phone in front of a work area with a computer on it#\n1\na person using a cell phone in front of a stupid computer#\n-1\nthe broken computer could of warned us about the future cold front#\n-1\n"
     ]
    }
   ],
   "source": [
    "# l=0\n",
    "# for k in senti_cap_dict.keys():\n",
    "#     v= senti_cap_dict[k]\n",
    "#     print(k+\"#\")\n",
    "#     print(v)\n",
    "#     l+=1\n",
    "#     if(l>20):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 with a paper cutter and assorted crafting supplies in view on a table one can easily imagine making interesting wonderful nice beautiful important thing .\n\" .\n8863 train-cap_length\n8863 train-senti_score\n"
     ]
    }
   ],
   "source": [
    "\"\"\"check result\"\"\"\n",
    "# for i in range(0,l1,5):\n",
    "#     avg= np.mean(senti_score[i:i+5])\n",
    "#     variance= np.var(senti_score[i:i+5])\n",
    "#     senti_avg.append(avg)\n",
    "#     senti_var.append(variance)\n",
    "# \n",
    "# plt.figure()\n",
    "# _= plt.hist(senti_score,bins='auto')\n",
    "# plt.title(\"score\")\n",
    "# \n",
    "# plt.figure()\n",
    "# _= plt.hist(senti_var,bins='auto')\n",
    "# # plt.show()\n",
    "# plt.title(\"variance\")\n",
    "# \n",
    "# # plt.figure()\n",
    "# # _= plt.hist(senti_avg,bins='auto')\n",
    "# # plt.show()\n",
    "# \n",
    "# plt.figure()\n",
    "# senticap_sentiment=[]\n",
    "# for i in range(len(senticap_list)):\n",
    "#     senticap_sentiment.append(senticap_list[i][1])\n",
    "# _=plt.hist(senticap_sentiment,bins='auto')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= os.path.abspath('.')+'/data/val2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vec=[]\n",
    "sentiment_vec=[]\n",
    "caption_vec=[]\n",
    "\n",
    "for item in senticap_list:\n",
    "    img= item[0]\n",
    "    if item[1]==1:\n",
    "        senti=1\n",
    "    elif item[1]==0:\n",
    "        senti=-1\n",
    "    else:\n",
    "        senti=0\n",
    "        \n",
    "    caption= item[2]\n",
    "    # print(caption+\"#\")\n",
    "    if caption in senti_cap_dict.keys():\n",
    "        new_senti= senti_cap_dict[caption]\n",
    "    else:\n",
    "        new_senti= 0\n",
    "    img= PATH + img \n",
    "    caption= '<S> ' + caption + ' </S>'\n",
    "    image_vec.append(img)\n",
    "    sentiment_vec.append(new_senti)\n",
    "    caption_vec.append(caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> a plate of delicious food including french fries </S>\n"
     ]
    }
   ],
   "source": [
    "num_examples = 6000\n",
    "start=0\n",
    "\n",
    "image_name_train= image_vec[:num_examples]\n",
    "senti_train= sentiment_vec[:num_examples]\n",
    "caption_train= caption_vec[:num_examples]\n",
    "print(caption_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img= cv2.imread(image_path)\n",
    "    img= cv2.resize(img,(299,299))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "units= 128\n",
    "embedding_size=128\n",
    "BATCH_SIZE=4\n",
    "vocab_size=500\n",
    "max_length=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = vocab_size-1\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(caption_train)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 110, 7, 190, 30, 1, 1, 1, 4, 5]\n[  3   2 110   7 190  30   1   1   1   4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(caption_train)\n",
    "cap_vector_train = keras.preprocessing.sequence.pad_sequences(train_seqs,dtype='int32', padding='post',truncating='post',maxlen=max_length,value=0)\n",
    "\n",
    "print(train_seqs[0])\n",
    "print(cap_vector_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_img= []\n",
    "# \n",
    "# k=0\n",
    "# for i in tqdm(image_name_train):\n",
    "#     img= load_image(i)\n",
    "#     input_img.append(img)\n",
    "#     \n",
    "# input_img=np.array(input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW0921 00:14:06.240314 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:06.259203 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:06.262452 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:06.272525 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:06.273079 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:07.761358 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:07.875344 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:14:08.253289 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 2048)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=inception_v3.InceptionV3(include_top=False,weights=None)\n",
    "weights_path = './inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "# image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "#                                                 weights='imagenet')\n",
    "model.load_weights(weights_path)\n",
    "print(model.output_shape)\n",
    "for layer in model.layers[:311]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 2048)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_input = model.layers[0].input\n",
    "hidden_layer = model.layers[-1].output\n",
    "\n",
    "image_model = keras.Model(new_input, hidden_layer)\n",
    "print(image_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 11, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0920 18:55:30.243614 139620787287872 deprecation.py:323] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# image_input=tf.placeholder(tf.float32,shape=(None,299,299,3))\n",
    "# seq_input= tf.placeholder(tf.int32,shape=(None,max_length))\n",
    "# senti_input= tf.placeholder(tf.float32,shape=(None))\n",
    "# \n",
    "# image_feature=image_model(image_input) \n",
    "# image_feature= Flatten()(image_feature)\n",
    "# embedded_image= keras.layers.Dense(embedding_size)(image_feature)\n",
    "# seq_temp= keras.backend.one_hot(seq_input, vocab_size)\n",
    "# embedded_seq= keras.layers.Dense(units)(seq_temp)\n",
    "# x= tf.concat([tf.expand_dims(embedded_image, 1), embedded_seq], axis=1)\n",
    "# print(x.shape)\n",
    "# x= tf.reshape(x,shape=(-1, max_length+1,units))\n",
    "# output = keras.layers.LSTM(units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=False)(x)\n",
    "# \n",
    "# \n",
    "# lstm_output= tf.reshape(output,(-1,units))\n",
    "# logits= keras.layers.Dense(vocab_size,activation='softmax')(lstm_output)\n",
    "# # logits_2= logits[:,1:max_length,vocab_size]\n",
    "# target=seq_input[:,0:max_length]\n",
    "# \n",
    "# loss= tf.reduce_mean(keras.losses.sparse_categorical_crossentropy(target,logits))\n",
    "# \n",
    "# mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "# mask = tf.cast(mask, dtype=loss.dtype)\n",
    "# batch_loss= loss * mask\n",
    "# \n",
    "# train_step= tf.train.AdamOptimizer().minimize(batch_loss)\n",
    "# inference_step= tf.argmax(logits,axis=-1)\n",
    "# \n",
    "# # word= [tokenizer.index_word[x] for x in inference_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image_name_train\n",
    "# # senti_train\n",
    "# # cap_vector_train\n",
    "# sess= tf.Session()\n",
    "# init_op= tf.global_variables_initializer()\n",
    "# with sess.as_default():\n",
    "#     init_op.run()\n",
    "#     batch_size= BATCH_SIZE\n",
    "#     start=0\n",
    "#     for i in tqdm(range(10)):\n",
    "#         input_imgs=[]\n",
    "#         for j in range(batch_size):\n",
    "#             img= load_image( image_name_train[start+j])\n",
    "#             input_imgs.append(img)\n",
    "#         input_imgs= np.array(input_imgs)\n",
    "#         input_sentis= senti_train[start:start+batch_size]\n",
    "#         input_sequences= cap_vector_train[start:start+batch_size]\n",
    "#         start+=batch_size\n",
    "#             \n",
    "# \n",
    "#         train_step.run (feed_dict={image_input:input_imgs,seq_input: input_sequences, senti_input: input_sentis})\n",
    "#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = keras.losses.sparse_categorical_crossentropy(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input=Input(shape=(299,299,3),dtype=tf.float32)\n",
    "seq_input= Input(shape=(max_length,),dtype=tf.int32)\n",
    "senti_input= Input(shape=(1,),dtype=tf.int32)\n",
    "\n",
    "image_feature=image_model(image_input) \n",
    "image_feature= Flatten()(image_feature)\n",
    "embedded_image= keras.layers.Dense(embedding_size)(image_feature)\n",
    "\n",
    "embedded_seq= Embedding(input_dim=1,output_dim=units)(seq_input)\n",
    "\n",
    "func2= Lambda (lambda w: tf.expand_dims(w, 1))\n",
    "expanded_image=func2(embedded_image)\n",
    "\n",
    "x= Concatenate(axis=1)([expanded_image, embedded_seq])\n",
    "# print(x.shape)\n",
    "x= Reshape(( max_length+1,units))(x)\n",
    "output = keras.layers.LSTM(units, return_sequences=True,\n",
    "                           return_state=False,\n",
    "                           recurrent_initializer='glorot_uniform')(x)\n",
    "\n",
    "\n",
    "# lstm_output= tf.reshape(output,(-1,units))\n",
    "lstm_output= Reshape((-1,units))(output)\n",
    "logits= keras.layers.Dense(vocab_size,activation='softmax')(lstm_output)\n",
    "func3= Lambda (lambda w: tf.slice(w,[0,0,0],[-1,max_length,-1]))\n",
    "logits_2= func3(logits)\n",
    "\n",
    "# target=seq_input[:,0:max_length]\n",
    "# \n",
    "# loss= tf.reduce_mean(keras.losses.sparse_categorical_crossentropy(target,logits))\n",
    "# \n",
    "# mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "# mask = tf.cast(mask, dtype=loss.dtype)\n",
    "# batch_loss= loss * mask\n",
    "# \n",
    "# train_step= tf.train.AdamOptimizer().minimize(batch_loss)\n",
    "# inference_step= tf.argmax(logits,axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:15:02.001816 139620787287872 deprecation_wrapper.py:119] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 500)\n"
     ]
    }
   ],
   "source": [
    "new_model= keras.Model([image_input,seq_input,senti_input],logits_2)\n",
    "new_model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam')\n",
    "\n",
    "print(new_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 00:19:56.354986 139620787287872 deprecation.py:323] From /home/yiyang/anaconda3/envs/caption/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 299, 299, 3)\n(1000, 10)\n(1000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node model_1/conv2d_1/convolution}}]]\n\t [[loss/mul/_3887]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node model_1/conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-50aeb8b158da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_sentis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/caption/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/caption/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/caption/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/caption/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node model_1/conv2d_1/convolution}}]]\n\t [[loss/mul/_3887]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node model_1/conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# for i in tqdm(range(1)):\n",
    "input_imgs=[]\n",
    "for j in range(1000):\n",
    "    img= load_image( image_name_train[start+j])\n",
    "    input_imgs.append(img)\n",
    "input_imgs= np.array(input_imgs)\n",
    "input_sentis= np.array(senti_train[:1000])\n",
    "input_sequences= np.array(cap_vector_train[:1000])\n",
    "target= np.expand_dims(input_sequences,axis=-1)\n",
    "print(input_imgs.shape)\n",
    "print(input_sequences.shape)\n",
    "print(input_sentis.shape)\n",
    "new_model.fit(x=[input_imgs,input_sequences,input_sentis],y=target, batch_size=1,verbose=1,epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(x=input_x,y=input_y,batch_size=BATCH_SIZE,verbose=1,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(keras.Model):\n",
    "    def __init__(self, embedding_size, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.img_embedding_size= embedding_size\n",
    "        \n",
    "        self.vocab_size= vocab_size\n",
    "    \n",
    "        self.units = units\n",
    "        #seq emnedding\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.lstm = keras.layers.LSTM(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.cnn_fc1 = keras.layers.Dense(self.img_embedding_size)\n",
    "        \n",
    "        self.lstm_fc1 = keras.layers.Dense(self.units)\n",
    "        self.lstm_fc2 = keras.layers.Dense(self.vocab_size) \n",
    "        \n",
    "    def call(self, image_input, seq_input, senti_input):\n",
    "        embedded_image= self.cnn_fc1(image_input)\n",
    "        \n",
    "        embedded_seq= self.lstm_fc1(seq_input)\n",
    "        \n",
    "        x= tf.concat([tf.expand_dims(embedded_image, 1), embedded_seq], axis=1)\n",
    "        \n",
    "        output, state= self.lstm(x)\n",
    "        lstm_output= tf.reshape(output,(-1,self.units))\n",
    "        \n",
    "        \n",
    "        logits= self.lstm_fc2(lstm_output)\n",
    "        return logits, state \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
